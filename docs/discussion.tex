\section{Discussão}

Os resultados obtidos evidenciam que a implementação CUDA apresentou o melhor desempenho, mas houve uma falha na obtenção de mais dados, o que limitou a análise mais aprofundada da escalabilidade da solução. No entanto, dentro dos experimentos realizados, verificou-se que um tamanho de bloco igual a 8 proporcionou o melhor desempenho. Esse comportamento pode ser explicado pelo balanceamento entre a ocupação da GPU e a minimização da sobrecarga associada à troca de contexto entre warps. Blocos muito pequenos podem não explorar eficientemente a capacidade dos multiprocessadores da GPU, enquanto blocos muito grandes podem causar contenção de recursos e maior latência na comunicação entre threads.

No caso do MPI, a configuração com 6 processos demonstrou o melhor desempenho entre as implementações paralelas CPU. Isso pode ser atribuído ao fato de que um número adequado de processos permite um balanceamento eficiente da carga de trabalho entre os núcleos disponíveis, minimizando a sobrecarga de comunicação e sincronização. Por outro lado, observou-se uma instabilidade nos tempos de execução do MPI, o que pode estar relacionado a flutuações na comunicação entre processos e à alocação dinâmica de recursos do sistema operacional, especialmente em clusters compartilhados.

Outro ponto relevante foi a queda de desempenho tanto no MPI quanto no OpenMP conforme o número de processos ou threads aumentava. Esse fenômeno pode ser explicado pelo aumento da contenção de memória compartilhada e pelo overhead gerado pela sincronização entre as threads/processos. No OpenMP, à medida que mais threads são adicionadas, o benefício do paralelismo é progressivamente reduzido pela latência no acesso à memória e pela competição por cache L3. No MPI, um número excessivo de processos pode aumentar o custo da comunicação interprocessos, tornando a execução menos eficiente.

Curiosamente, a versão sequencial otimizada com a flag -O3 do GCC superou a implementação OpenMP com 12 threads. Isso sugere que a otimização agressiva aplicada pelo compilador, incluindo vectorization e reordering de instruções, foi suficiente para melhorar o desempenho sem a necessidade do overhead de gerenciamento de múltiplas threads. Esse resultado reforça a importância de considerar otimizações a nível de compilação antes de recorrer a técnicas de paralelismo explícito, uma vez que a simples ativação de OpenMP não garante automaticamente um ganho de desempenho significativo.

Diante dessas observações, futuros trabalhos podem explorar ajustes mais refinados na granularidade do paralelismo, estratégias de escalonamento de threads e otimizações específicas para minimizar a sobrecarga de comunicação em implementações MPI e OpenMP. Além disso, uma análise mais detalhada da configuração da memória na GPU poderia permitir um entendimento mais preciso sobre os limites da implementação CUDA e sua escalabilidade em diferentes tamanhos de problemas.