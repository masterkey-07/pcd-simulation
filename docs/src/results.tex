\section{Resultados e Discussão}

Nesta seção, são apresentados os resultados obtidos para cada técnica de otimização e paralelização aplicada, assim, com os resultados do algoritmo sem otimização, será feito uma análise de speedup para cada técnica.

Para cada técnica, foi conduzida uma série de simulações utilizando combinações variadas de tamanhos de matriz e quantidades de iterações, conforme descrito a seguir:

\begin{itemize}
    \item \textbf{Tamanho da Matriz:} 2000, 4000, 6000
    \item \textbf{Quantidade de Iterações:} 500, 1000, 1500
\end{itemize}

\subsection{Resultados do Algoritmo Compilado com a Flag -O3}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/O3 - SpeedUp.png}
\end{figure}

\subsection{Resultados do Algoritmo Paralelizado com OpenMP}

No gráfico abaixo, sobre a média de SpeedUp, os dois casos com o melhor SpeedUp é com 12 e 6 threads, invés de ser 12 e 10 threads, apesar de não ser encontrado um motivo para isso, pode ser que na configuração de 6 processos tem o menor overhead em comparação ao tempo ganho com a paralelização.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/OMP - SpeedUp.png}
\end{figure}

No gráfico abaixo, sobre a performance, é relevante a queda de performance conforme mais threads são usadas, indicando assim que quanto mais threads menos eficiente é o uso das CPUs.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/OMP - Performance.png}
\end{figure}

\subsection{Resultados do Algoritmo Paralelizado com OpenMPI e com Otimização de Compilação}

No gráfico abaixo, sobre a média de SpeedUp, os dois casos com o melhor SpeedUp é com 6 (a melhor) e 10 processos, invés de ser 12 e 10 processos, isso pode ocorrer por diminuir a quantidade de sincronização necessária entre processos enquanto há uma divisão adequada de trabalho entre os processos.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/MPI - SpeedUp.png}
\end{figure}

No gráfico abaixo, sobre a performance, é relevante notar que em comparação a paralelização com OpenOMP, a performance com MPI sempre está acima que 1, enquanto do OpenOMP esteve em sua maioria, abaixo de 1, assim indicando uma ótima utilização dos recursos do computador quando a atividade é dividida entre processos que em threads.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/MPI - Performance.png}
\end{figure}

\subsection{Resultados do Algoritmo Adaptado para CUDA}
\subsubsection{Configuração do Ambiente de Execução}
\begin{itemize}
    \item Máquina: Ambiente Virtual do Google Colab
    \item CPU:
    \item Memória RAM: 16 GB
    \item GPU: T4
    \item Compilador: NVCC (NVIDIA CUDA Compiler)
\end{itemize}

No gráfico abaixo de SpeedUp, comparando cada caso com um tamanho de bloco diferente, é notável que com 16 de tamanho de bloco é o que apresentou o melhor SpeedUp. Esse comportamento pode ser explicado pelo balanceamento entre a ocupação da GPU e a minimização da sobrecarga associada à troca de contexto entre warps. Blocos muito pequenos podem não explorar eficientemente a capacidade dos multiprocessadores da GPU, enquanto blocos muito grandes podem causar contenção de recursos e maior latência na comunicação entre threads.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/CUDA - SpeedUp.png}
\end{figure}

\subsection{Resultados da Concentração e de Assertividade}

Abaixo, é apresentado os resultados da concentração no meio em qualquer grade da matriz em relação a quantidade de iterações.

\begin{table}[H]
    \centering
    \caption{Resultados Corretos da Concentração (Com Matriz de Tamanho 2000, 4000 ou 6000)}
    \label{tab:exemplo}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Iterações} & Concentração no Meio \\
        \midrule
        500                & 0.216512             \\
        1000               & 0.095045             \\
        1500               & 0.058997             \\
        \bottomrule
    \end{tabular}
\end{table}

Nos próximos três gráficos, é apresentado em visão de dispersão os resultados alcançados pelos métodos em cada quantidade de iterações, apesar de o algoritmo do CUDA apenas entrar no caso de 1000 iterações, todos eles tem apresentado resultados iguais ao original, todavia, alguns casos com o algoritmo do MPI estiveram fora do padrão na terceira casa do decimal.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/A.png}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/B.png}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/C.png}
\end{figure}

\subsection{Comparativo com todos os Métodos}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../assets/Final.png}
\end{figure}

Os resultados obtidos evidenciam que a implementação CUDA apresentou o melhor desempenho, mas houve uma falha na obtenção de mais dados, o que limitou a análise mais aprofundada da escalabilidade da solução.

Entre as otimizações com CPU, OpenMPI apresentou o melhor resultado utilizando de 6 processos. E curiosamente, a versão sequencial otimizada com a flag -O3 do GCC superou a implementação OpenMP com 12 threads. Isso sugere que a otimização agressiva aplicada pelo compilador, foi suficiente para melhorar o desempenho sem a necessidade do overhead de gerenciamento de múltiplas threads, indicando que problemas de itensivo iteração pode ser otimizados em compilação.

Por fim, os resultados dos modelos tem sido extremamente fiéis ao resultado original, todavia, há alguns casos que o algoritmo do MPI tem errado na concentração final no meio da grade, tal erro pode ser por conta de algum problema de sincronização que ocorreu durante a execução.